{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gH7AvTPneQch",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76829ab6-d602-464f-91bd-f3e14a5eb9e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.24.7-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.24.6 (from PyMuPDF)\n",
            "  Downloading PyMuPDFb-1.24.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
            "Successfully installed PyMuPDF-1.24.7 PyMuPDFb-1.24.6\n"
          ]
        }
      ],
      "source": [
        "!pip install PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-b-Z1LcwkLh_",
        "outputId": "14b373c6-6e63-43a2-f5d4-db65556d6b3d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m225.3/232.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import fitz  # PyMuPDF for PDF handling\n",
        "from nltk import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict\n",
        "from heapq import nlargest\n",
        "import nltk\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "\n",
        "# Ensure necessary NLTK data is downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Folder where PDF files are located\n",
        "pdf_folder = '/content/drive/MyDrive/Pdf'\n",
        "\n",
        "# Output file for summaries\n",
        "output_file = 'summaries.txt'\n",
        "\n",
        "# Function to read PDF files and extract text from all pages\n",
        "def read_pdf_files(folder_path):\n",
        "    texts = []\n",
        "    filenames = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.pdf'):\n",
        "            filepath = os.path.join(folder_path, filename)\n",
        "            try:\n",
        "                doc = fitz.open(filepath)\n",
        "                text = ''\n",
        "                for page_num in range(len(doc)):\n",
        "                    page = doc.load_page(page_num)\n",
        "                    text += page.get_text()\n",
        "                if text.strip():  # Ensure extracted text is not empty\n",
        "                    texts.append(text)\n",
        "                    filenames.append(filename)\n",
        "                else:\n",
        "                    print(f\"Warning: {filename} does not contain readable text.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading {filename}: {str(e)}\")\n",
        "    return texts, filenames\n",
        "\n",
        "# Function to summarize text using NLTK\n",
        "def summarize_text_nltk(text, max_words=None, num_sentences=5):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text.lower())\n",
        "    word_frequencies = defaultdict(int)\n",
        "\n",
        "    for word in words:\n",
        "        if word not in stop_words and word.isalnum():\n",
        "            word_frequencies[word] += 1\n",
        "\n",
        "    sentence_list = sent_tokenize(text)\n",
        "    sentence_scores = defaultdict(int)\n",
        "\n",
        "    for sentence in sentence_list:\n",
        "        for word in word_tokenize(sentence.lower()):\n",
        "            if word in word_frequencies:\n",
        "                sentence_scores[sentence] += word_frequencies[word]\n",
        "\n",
        "    if max_words:\n",
        "        current_words = 0\n",
        "        summary_sentences = []\n",
        "        for sentence in sentence_list:\n",
        "            if current_words < max_words:\n",
        "                summary_sentences.append(sentence)\n",
        "                current_words += len(word_tokenize(sentence))\n",
        "            else:\n",
        "                break\n",
        "    else:\n",
        "        summary_sentences = nlargest(num_sentences, sentence_scores, key=sentence_scores.get)\n",
        "\n",
        "    summary = ' '.join(summary_sentences)\n",
        "    return summary\n",
        "\n",
        "# Function to summarize text using BART model from Transformers\n",
        "def summarize_text_bart(text, max_length=150):\n",
        "    tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "    model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "    inputs = tokenizer([text], max_length=max_length, return_tensors='pt', truncation=True)\n",
        "    summary_ids = model.generate(inputs['input_ids'], num_beams=4, min_length=30, max_length=150, length_penalty=2.0, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return summary\n",
        "\n",
        "# Function to calculate time-accurancy summary length\n",
        "def time_accurancy_summary(text, words_per_minute, minutes):\n",
        "    target_words = words_per_minute * minutes\n",
        "    sentence_list = sent_tokenize(text)\n",
        "    current_words = 0\n",
        "    summary_sentences = []\n",
        "\n",
        "    for sentence in sentence_list:\n",
        "        if current_words < target_words:\n",
        "            summary_sentences.append(sentence)\n",
        "            current_words += len(word_tokenize(sentence))\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    summary = ' '.join(summary_sentences)\n",
        "    return summary\n",
        "\n",
        "# Function to summarize each page using NLTK\n",
        "def summarize_each_page_nltk(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    sentences = sent_tokenize(text)\n",
        "    summaries = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence.lower())\n",
        "        filtered_words = [word for word in words if word.isalnum() and word not in stop_words]\n",
        "        summary = ' '.join(filtered_words)\n",
        "        summaries.append(summary)\n",
        "\n",
        "    return summaries\n",
        "\n",
        "# Read PDF files and extract texts\n",
        "texts, filenames = read_pdf_files(pdf_folder)\n",
        "\n",
        "# Initialize dictionary to store time accuracies\n",
        "time_accuracies = {}\n",
        "\n",
        "# Open output file to write summaries\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    # Summarize each PDF file\n",
        "    for count, (text, filename) in enumerate(zip(texts, filenames), start=1):\n",
        "        f.write(f\"Count of PDF file: {count}\\n\")\n",
        "        f.write(f\"Filename: {filename}\\n\\n\")\n",
        "        f.write(f\"Full text from {filename}:\\n\")\n",
        "        f.write(text + '\\n\\n')\n",
        "\n",
        "        # Determine if the document is a book (over 200 pages)\n",
        "        num_pages = len(fitz.open(os.path.join(pdf_folder, filename)))\n",
        "        if num_pages > 200:\n",
        "            f.write(f\"\\nSummarizing all pages for {filename} using NLTK:\\n\")\n",
        "            nltk_summary = summarize_text_nltk(text)\n",
        "            f.write(nltk_summary + '\\n\\n')  # Using NLTK for summarization\n",
        "\n",
        "            # Summarize using BART model\n",
        "            f.write(f\"\\nSummarizing all pages for {filename} using BART model:\\n\")\n",
        "            bart_summary = summarize_text_bart(text)\n",
        "            f.write(bart_summary + '\\n\\n')\n",
        "\n",
        "            # Time-accurancy summarization example (30 minutes at 200 words per minute)\n",
        "            words_per_minute = 200\n",
        "            minutes = 30\n",
        "            f.write(f\"\\nTime-accurancy Summarization for {filename}:\\n\")\n",
        "            time_summary = time_accurancy_summary(text, words_per_minute, minutes)\n",
        "            f.write(time_summary + '\\n\\n')\n",
        "\n",
        "            # Summarize each page using NLTK\n",
        "            f.write(f\"\\nSummarizing each page for {filename} using NLTK:\\n\")\n",
        "            page_summaries = summarize_each_page_nltk(text)\n",
        "            for i, summary in enumerate(page_summaries, start=1):\n",
        "                f.write(f\"Page {i} summary: {summary}\\n\")\n",
        "\n",
        "            # Calculate accuracy for time-accurancy summary method\n",
        "            word_count = len(word_tokenize(text))\n",
        "            time_accuracy = len(word_tokenize(time_summary)) / word_count * 100\n",
        "\n",
        "            # Store time accuracy in dictionary\n",
        "            time_accuracies[filename] = time_accuracy\n",
        "\n",
        "            # Print separator\n",
        "            f.write(\"\\n--------------------------------------------------\\n\\n\")\n",
        "\n",
        "        else:\n",
        "            f.write(f\"\\n{filename} is less than 200 pages, skipping summarization.\\n\")\n",
        "            f.write(\"\\n--------------------------------------------------\\n\\n\")\n",
        "\n",
        "    # Write total count of PDF files processed\n",
        "    f.write(f\"Total PDF files processed: {len(filenames)}\\n\\n\")\n",
        "\n",
        "# Print overall accuracies for each PDF file\n",
        "with open(output_file, 'a', encoding='utf-8') as f:\n",
        "    f.write(\"Overall Accuracies:\\n\")\n",
        "    for filename in filenames:\n",
        "        f.write(f\"{filename}:\\n\")\n",
        "        if filename in time_accuracies:\n",
        "            f.write(f\"Time-accurancy Summary: {time_accuracies[filename]:.2f}%\\n\")\n",
        "        f.write(\"\\n--------------------------------------------------\\n\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hFkoCEH28j4",
        "outputId": "f8352dda-025c-4081-f3b6-8c31464ba5b5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Out of the Shadows_ Understanding Sexual Addiction ( PDFDrive ).pdf does not contain readable text.\n",
            "Warning: The Power of Now_ A Guide to Spiritual Enlightenment ( PDFDrive ).pdf does not contain readable text.\n",
            "Warning: The Essential Rumi by Coleman Barks ( PDFDrive ).pdf does not contain readable text.\n",
            "Warning: Muhammad the Prophet by Maulana Muhammad Ali ( PDFDrive ).pdf does not contain readable text.\n"
          ]
        }
      ]
    }
  ]
}